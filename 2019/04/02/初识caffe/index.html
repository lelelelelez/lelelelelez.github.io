<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    初识caffe |
    
    记录者</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  
    <link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">
  
  <script src="/js/pace.min.js"></script>
</head>
</html>
<body>
<main class="content">
  <section class="outer">
  <article id="post-初识caffe" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      初识caffe
    </h1>
  

      </header>
    

    
      <div class="article-meta">
        <a href="/2019/04/02/初识caffe/" class="article-date">
  <time datetime="2019-04-02T12:24:18.000Z" itemprop="datePublished">2019-04-02</time>
</a>
        
      </div>
    

    <div class="article-entry" itemprop="articleBody">
      

      

      
        <p>​    因工作需要，学习caffe。本文真的是小白写的总结，希望不要见笑。在写的过程中，其实还有很多东西都不太懂，希望在实践中可以加深了解。</p>
<a id="more"></a>
<h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h3><p>​    caffe是一个清晰而高效的深度学习框架，纯粹的C++/CUDA架构，支持命令行、Python和MATLAB接口，可以在CPU和GPU直接无缝切换；　</p>
<p>​    caffe的主要优势：（1）CPU与GPU的无缝切换；（2）模型与优化都是通过配置文件来设置，无需代码；</p>
<h3 id="2-caffe安装（Mac）"><a href="#2-caffe安装（Mac）" class="headerlink" title="2. caffe安装（Mac）"></a>2. caffe安装（Mac）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">brew install -vd snappy leveldb gflags glog szip lmdb</span><br><span class="line">brew tap homebrew/science</span><br><span class="line"><span class="meta">#</span><span class="bash">报error:Error: homebrew/science was deprecated. This tap is now empty as all its formulae were migrated</span></span><br><span class="line"><span class="meta">#</span><span class="bash">寻找一下：</span></span><br><span class="line">brew search science</span><br><span class="line"><span class="meta">#</span><span class="bash">安装第一个</span></span><br><span class="line"></span><br><span class="line">brew install opencv hdf5</span><br><span class="line">brew install protobuf boost boost-python</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">安装brew</span></span><br><span class="line">/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"</span><br><span class="line"><span class="meta">#</span><span class="bash">安装cmake  </span></span><br><span class="line"><span class="meta">#</span><span class="bash">https://cmake.org/download/</span></span><br><span class="line"><span class="meta">#</span><span class="bash">下载dmg 直接安装</span></span><br><span class="line"><span class="meta">#</span><span class="bash">安装</span></span><br><span class="line">for x in snappy leveldb gflags glog szip hdf5 lmdb</span><br><span class="line">do</span><br><span class="line">    brew install $x;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">brew install homebrew/cask/data-science-studio</span><br><span class="line">brew install opencv hdf5</span><br><span class="line">brew install protobuf boost boost-python</span><br><span class="line"><span class="meta">#</span><span class="bash">安装caffe</span></span><br><span class="line">git clone https://github.com/BVLC/caffe</span><br><span class="line">cp Makefile.config.example Makefile.config</span><br><span class="line">vim Makefile.config</span><br><span class="line">CPU_ONLY := 1 #因为机器只有CPU</span><br><span class="line"><span class="meta">#</span><span class="bash">编译</span></span><br><span class="line">make all</span><br></pre></td></tr></table></figure>
<p><img src="/images/af2dee7f95f6c904be0f2bd4237c362a.png" alt="af2dee7f95f6c904be0f2bd4237c362a"></p>
<p>出现以上问题：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim Makefile</span><br><span class="line"><span class="meta">#</span><span class="bash">添加</span></span><br><span class="line">CXXFLAGS += -std=c++11</span><br></pre></td></tr></table></figure>
<p>即可解决。</p>
<h3 id="4-caffe的总体框架"><a href="#4-caffe的总体框架" class="headerlink" title="4. caffe的总体框架"></a>4. caffe的总体框架</h3><p>Caffe主要由Blob，Layer，Net 和 Solver这几个部分组成。</p>
<p><img src="/images/image-20190331122759639.png" alt="image-20190331122759639"></p>
<blockquote>
<p>Blob：通常是用来存储数据的，包括训练数据，以及各个网络层的自身参数（权值，偏置和剃度等）。一般是四位矩阵，也就是我们常说的NCHW。而且Blob还可以进行网络层的数据传递。</p>
<p>Layer：各个层的一种抽象，比如卷积层，激活函数等。每个Layer都实现了前向传播和后向传播。</p>
<p>Net：所有的Layer构成一个Net，也就是我们常说的网络模型。</p>
<p>Solver：针对Net的训练，每个Solver都包括一个训练网络对象和一个测试网络对象。</p>
</blockquote>
<h3 id="5-caffe训练流程"><a href="#5-caffe训练流程" class="headerlink" title="5. caffe训练流程"></a>5. caffe训练流程</h3><h4 id="5-1-准备数据"><a href="#5-1-准备数据" class="headerlink" title="5.1 准备数据"></a>5.1 准备数据</h4><p>从网上下载了500张图片，分为5大类，每个类100张，并进行标记，相当于贴标签。每类我们选择80张作为训练图片，20张作为测试图片。</p>
<h4 id="5-1-数据格式化"><a href="#5-1-数据格式化" class="headerlink" title="5.1 数据格式化"></a>5.1 数据格式化</h4><p>将下载好的图片，转换成caffe能识别的db（leveldb或lmdb）格式。</p>
<p>caffe提供了转换cpp，在<code>caffe/tools</code>目录下的<code>convert_imageset.cpp</code>，编译后的可执行文件是<code>caffe/build/tools</code>下<code>convert_imageset</code></p>
<p>执行命令：</p>
<p><strong>convert_imageset  [FLAGS]  ROOTFOLDER/  LISTFILE  DB_NAME</strong></p>
<p>四个参数：</p>
<p>　　<strong>FLAGS:</strong> 图片参数组，后面详细介绍；</p>
<p>　    <strong>ROOTFOLDER/:</strong> 图片存放的绝对路径，从linux系统根目录开始；</p>
<p>​    　<strong>LISTFILE:</strong> 图片文件列表清单，也就是标签文件，一般为一个txt文件，一行一张图片；</p>
<p>　　<strong>DB_NAME:</strong> 最终生成的db文件存放目录。</p>
<p>第二个和第四个看自己喜好，先说怎么生成第三个，也就是怎么生成标签文件。写一个shell脚本来自动生成。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/usr/bin/env sh</span></span><br><span class="line">DATA=../../data/re</span><br><span class="line">MY=.</span><br><span class="line">echo "Create train.txt..."</span><br><span class="line">rm -rf $MY/train.txt</span><br><span class="line">touch $MY/train.txt</span><br><span class="line">for i in 3 4 5 6 7</span><br><span class="line">do</span><br><span class="line">find $DATA/train -name $i*.jpg | cut -d '/' -f4-5 | sed "s/$/ $i/"&gt;&gt;$MY/train.txt</span><br><span class="line">done</span><br><span class="line">echo "Create test.txt..."</span><br><span class="line">rm -rf $MY/test.txt</span><br><span class="line">touch $MY/test.txt</span><br><span class="line">for i in 3 4 5 6 7</span><br><span class="line">do</span><br><span class="line">find $DATA/test -name $i*.jpg | cut -d '/' -f4-5 | sed "s/$/ $i/"&gt;&gt;$MY/test.txt</span><br><span class="line">done</span><br><span class="line">echo "All done"</span><br></pre></td></tr></table></figure>
<p>最后产生的标签文件如下：</p>
<p><img src="/images/image-20190331131137046.png" alt="image-20190331131137046"></p>
<p>该txt文件即可作为第三个参数来使用。</p>
<p>然后看第一个参数FLAGS-参数组，它通常有以下内容：</p>
<p>　　　　<strong>-gray:</strong> 是否以灰度图的方式打开图片。程序调用opencv库中的imread()函数来打开图片，默认为false</p>
<p>　　　　<strong>-shuffle:</strong> 是否随机打乱图片顺序。默认为false</p>
<p>　　　　<strong>-backend:</strong>需要转换成的db文件格式，可选为leveldb或lmdb,默认为lmdb</p>
<p>　　　　<strong>-resize_width/resize_height:</strong> 改变图片的大小。在运行中，要求所有图片的尺寸一致，因此需要改变图片大小。 程序调用opencv库的resize（）函数来对图片放大缩小，默认为0，不改变</p>
<p>　　　　<strong>-check_size:</strong> 检查所有的数据是否有相同的尺寸。默认为false,不检查</p>
<p>　　　　<strong>-encoded:</strong> 是否将原图片编码放入最终的数据中，默认为false</p>
<p>　　　　<strong>-encode_type:</strong> 与前一个参数对应，将图片编码为哪一个格式：‘png’,’jpg’……</p>
<p>通过以上解释，我们可以执行以下命令：</p>
<p><code>./build/tools/convert_imageset --resize_width=256 --resize_height=256 data/ examples/zclfile/train.txt examples/zclfile/img_train_lmdb</code></p>
<p>我执行该命令，遇到问题</p>
<ul>
<li><p>dyld: Library not loaded: @rpath/libhdf5.101.dylibzezd</p>
<p>解决方式：先找下libhdf5.101.dylib文件，find ~/ -name libhdf5.101.dylib</p>
<p>发现它在anaconda2/lib/libhdf5.101.dylib ，所以再去做个软链接即可：</p>
<p><code>ln -s anaconda2/lib/libhdf5.101.dylib /usr/local/lib/</code></p>
</li>
</ul>
<p>继续执行上述命令。又遇到问题：</p>
<ul>
<li><p>不能使用opencv（大概意思是这个）</p>
<p>解决方式：在caffe的Makefile.config文件中打开opencv</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">INCLUDE_DIRS</span><br><span class="line">USE_OPENCV := 1</span><br><span class="line"><span class="meta">#</span><span class="bash">注意看下使用的opencv版本</span></span><br><span class="line">OPENCV_VERSION := 3 #默认是3</span><br></pre></td></tr></table></figure>
<p>然后就需要重新编译caffe了，在根目录 make clean，make all。</p>
<p>好的，又遇到问题了</p>
</li>
<li><p>src/caffe/data_transformer.cpp:2:10: fatal error: ‘opencv2/core/core.hpp’ file not found</p>
<p>解决方式：找不到该文件？那我就找下 发现我安装的opencv的版本是4.0++，为了让他找到这个目录，但我就在Makefile.config的INCLUDE_DIRS加上这个文件所在的目录就好</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/local/Cellar/opencv/4.0.1/include/opencv4 </span><br><span class="line"><span class="meta">#</span><span class="bash">我的这个文件就在/usr/<span class="built_in">local</span>/Cellar/opencv/4.0.1/include/opencv4下 我感觉大部分都在这个目录下：/usr/<span class="built_in">local</span>/Cellar，所以遇到问题就去这边找就行</span></span><br></pre></td></tr></table></figure>
<p>继续make all，又遇到问题：</p>
</li>
<li><p>src/caffe/layers/window_data_layer.cpp:293:42: error: use of undeclared identifier ‘CV_LOAD_IMAGE_COLOR’  cv_img = cv::imread(image.first, CV_LOAD_IMAGE_COLOR);</p>
<p>解决方式：网络搜一下，呵，caffe不支持opencv4.0以上的版本。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim src/caffe/layers/window_data_layer.cpp</span><br><span class="line"><span class="meta">#</span><span class="bash">更换 CV_LOAD_IMAGE_COLOR为cv::IMREAD_COLOR</span></span><br><span class="line"> cv_img = cv::imread(image.first, cv::IMREAD_COLOR)</span><br></pre></td></tr></table></figure>
<p>继续make all。（可能还有一种方式就是安装opencv3，我安装了 但是没试）</p>
<p>又遇到问题，而且是四个问题。</p>
</li>
</ul>
<p><img src="/images/image-20190331143433558.png" alt="image-20190331143433558"></p>
<p>可以看出这些个问题都跟上面的问题差不多，替换就行</p>
<p> CV_LOAD_IMAGE_COLOR替换成 cv::IMREAD_COLOR</p>
<p>CV_LOAD_IMAGE_GRAYSCALE 替换成 cv::IMREAD_GRAYSCALE</p>
<p>继续编译。又遇到问题</p>
<p><img src="/images/image-20190331150215917.png" alt="image-20190331150215917"></p>
<ul>
<li><p>以上图</p>
<p>解决方式：我真的是找了好多种方式去解决这个问题，最后没想到只是需要加一句话在Makefile中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim Makefile</span><br><span class="line"><span class="meta">#</span><span class="bash">找到LIBRARIES在后面添加opencv_imgcodecs</span></span><br><span class="line">LIBRARIES += glog gflags protobuf boost_system boost_filesystem m hdf5_hl hdf5 opencv_imgcodecs</span><br></pre></td></tr></table></figure>
<p>然后就编译通过了！（终于，要哭了）</p>
</li>
</ul>
<p>继续格式化数据，OK，终于成功了。成功后，在你的目录下会生成img_test_lmdb和img_train_lmdb两个目录，里面放着图片转换后的lmdb文件。</p>
<h4 id="5-3-计算均值并保存"><a href="#5-3-计算均值并保存" class="headerlink" title="5.3 计算均值并保存"></a>5.3 计算均值并保存</h4><p>图片减去均值再训练，会提高训练速度和精度。同样的，caffe也提供了一个计算均值的文件<strong>compute_image_mean.cpp</strong>，直接用</p>
<p><code>./../../build/tools/compute_image_mean ./img_train_lmdb mean.binaryproto</code></p>
<p><strong>compute_image_mean.cpp</strong> 需要两个参数，第一个是训练数据的位置，第二个是设定均值文件的名字及保存路径。OK，生成了mean.binaryproto。</p>
<h4 id="5-4-创建模型"><a href="#5-4-创建模型" class="headerlink" title="5.4 创建模型"></a>5.4 创建模型</h4><p>caffe是通过打开.prototxt配置文件来定义网络模型。这里我们使用caffe自带的手写数字库MNIST例子的网络结构文件。目录位置：examples/mnist/lenet_train_test.prototxt</p>
<p>该模型就是有名的<strong>LeNet模型</strong>，结构图如下：</p>
<p><img src="/images/image-20190331160213720.png" alt="image-20190331160213720"></p>
<ul>
<li><p>数据层的分类</p>
<ul>
<li><p>数据层（输入层）</p>
<p>数据的预处理（如减去均值, 放大缩小, 裁剪和镜像等），通常在这一层设置参数实现。</p>
<p>举例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: "mnist"      #表示层名</span><br><span class="line">  type: "Data"       #表示层的类型</span><br><span class="line">  top: "data"        #top表示输出，bottom表示输入</span><br><span class="line">  top: "label"       #只有top，表示只有输出，无输入</span><br><span class="line"><span class="meta">  #</span><span class="bash">在数据层中，至少有一个命名为data的top。如果有第二个top，一般命名为label。 这种(data,label)配对是分类模型所必需的</span></span><br><span class="line">  include &#123;   #属于训练阶段的层，还是属于测试阶段的层，需要用include来指定，若没有include,表示既在训练模型中，又在测试模型中</span><br><span class="line">    phase: TRAIN      #表示仅在训练阶段起作用</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123; #数据预处理</span><br><span class="line">    scale: 0.00390625  #将图像像素值归一化</span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    source: "examples/mnist/mnist_train_lmdb"   #数据来源，一般会有来自数据库，内存，HDF5，图片等，具体情况具体分析</span><br><span class="line">    batch_size: 64                    #训练时每个迭代的输入样本数量</span><br><span class="line">    backend: LMDB                               #数据类型</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>视觉层</p>
<p>包括 Convolution, Pooling, Local Response Normalization (LRN), im2col等层。</p>
<p>举例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: "conv1"</span><br><span class="line">  type: "Convolution"</span><br><span class="line">  bottom: "data"               #输入是data</span><br><span class="line">  top: "conv1"                 #输出是卷积特征</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 1                 #权重参数w的学习率倍数</span><br><span class="line">  &#125;#lr_mult学习率 最终的学习率是这个数乘以solver.prototxt配置文件中的base_lr。如果有两个lr_mult, 则第一个表示权值的学习率，第二个表示偏置项的学习率。一般偏置项的学习率是权值学习率的两倍。</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 2                 #偏置参数b的学习率倍数</span><br><span class="line">  &#125;</span><br><span class="line">  convolution_param &#123;</span><br><span class="line">    num_output: 20  #卷积核个数（filter）</span><br><span class="line">    kernel_size: 5  #卷积核大小，如果卷积核的长和宽不等，需要用kernel_h和kernel_w分别设定</span><br><span class="line">    stride: 1 #卷积核步长，默认是1</span><br><span class="line">    weight_filler &#123;           #权重参数w的初始化方案，使用xavier算法</span><br><span class="line">      type: "xavier"</span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      type: "constant"       #偏置参数b初始化化为常数，一般为0</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">卷积：</span></span><br><span class="line">输入：n*c0*w0*h0</span><br><span class="line">输出：n*c1*w1*h1</span><br><span class="line">c1就是参数中的num_output</span><br><span class="line">w1=(w0+2*pad-kernel_size)/stride+1;</span><br><span class="line">h1=(h0+2*pad-kernel_size)/stride+1;</span><br><span class="line">如果设置stride为1，前后两次卷积部分存在重叠。如果设置pad=(kernel_size-1)/2,则运算后，宽度和高度不变。</span><br><span class="line"><span class="meta">#</span><span class="bash">池化：pooling</span></span><br><span class="line">输入：n*c*w0*h0</span><br><span class="line">输出：n*c*w1*h1</span><br><span class="line">和卷积层的区别就是其中的c保持不变</span><br><span class="line"> w1=(w0+2*pad-kernel_size)/stride+1;</span><br><span class="line"> h1=(h0+2*pad-kernel_size)/stride+1;</span><br><span class="line">如果设置stride为2，前后两次卷积部分不重叠。100*100的特征图池化后，变成50*50.</span><br></pre></td></tr></table></figure>
</li>
<li><p>激活层</p>
<p>在激活层中，对输入数据进行激活操作（实际上就是一种函数变换），是逐元素进行运算的。从bottom得到一个blob数据输入，运算后，从top输入一个blob数据。在运算过程中，没有改变数据的大小，即输入和输出的数据大小是相等的。</p>
<p>输入：n*c*h*w</p>
<p>输出：n*c*h*w</p>
<p>常用的激活函数有sigmoid, tanh,relu等</p>
<p>详情请看：<a href="http://www.cnblogs.com/denny402/p/5072507.html" target="_blank" rel="noopener">http://www.cnblogs.com/denny402/p/5072507.html</a></p>
</li>
<li><p>其他层</p>
<p>包括：softmax_loss层，Inner Product层，accuracy层，reshape层和dropout层及其它们的参数配置</p>
</li>
</ul>
</li>
</ul>
<h4 id="5-5-编写配置文件"><a href="#5-5-编写配置文件" class="headerlink" title="5.5 编写配置文件"></a>5.5 编写配置文件</h4><p>先看一下mnist中配置文件（一般是..solver.prototxt文件）：</p>
<figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">vim examples/mnist/lenet_solver.prototxt</span><br><span class="line">配置文件如下：</span><br><span class="line"># The train/test net protocol buffer definition</span><br><span class="line">net: <span class="string">"examples/mnist/lenet_train_test.prototxt"</span>   <span class="comment">//设置深度网络模型，就是介绍的模型。</span></span><br><span class="line"># test_iter specifies how many forward passes the test should carry out.</span><br><span class="line"># In the case of MNIST, we have test batch size <span class="number">100</span> and <span class="number">100</span> test iterations,</span><br><span class="line"># covering the full <span class="number">10</span>,<span class="number">000</span> testing images.</span><br><span class="line">test_iter: <span class="number">100</span>  <span class="comment">//这个要与test layer中的batch_size结合起来理解。mnist数据中测试样本总数为10000，一次性执行全部数据效率很低，因此我们将测试数据分成几个批次来执行，每个批次的数量就是batch_size。假设我们设置batch_size为100，则需要迭代100次才能将10000个数据全部执行完。因此test_iter设置为100。执行完一次全部数据，称之为一个epoch</span></span><br><span class="line"># Carry out testing every <span class="number">500</span> training iterations.</span><br><span class="line">test_interval: <span class="number">500</span>  <span class="comment">// 测试间隔。也就是每训练500次，才进行一次测试。  </span></span><br><span class="line"># The base learning rate, momentum and the weight decay of the network.</span><br><span class="line">base_lr: <span class="number">0.01</span></span><br><span class="line">momentum: <span class="number">0.9</span>    <span class="comment">//上一次梯度更新的权重</span></span><br><span class="line">weight_decay: <span class="number">0.0005</span>   <span class="comment">//权重衰减项，防止过拟合的一个参数。</span></span><br><span class="line"># The learning rate policy</span><br><span class="line">lr_policy: <span class="string">"inv"</span></span><br><span class="line">gamma: <span class="number">0.0001</span></span><br><span class="line">power: <span class="number">0.75</span>   <span class="comment">//这几个参数用于学习率的设置。只要是梯度下降法来求解优化，都会有一个学习率，也叫步长。base_lr用于设置基础学习率，在迭代的过程中，可以对基础学习率进行调整。怎么样进行调整，就是调整的策略，由lr_policy来设置。</span></span><br><span class="line"># Display every <span class="number">100</span> iterations</span><br><span class="line">display: <span class="number">100</span>   <span class="comment">//每100次迭代显示一次</span></span><br><span class="line"># The maximum number of iterations</span><br><span class="line">max_iter: <span class="number">10000</span>   <span class="comment">//最大迭代次数。这个数设置太小，会导致没有收敛，精确度很低。设置太大，会导致震荡，浪费时间。</span></span><br><span class="line"># snapshot intermediate results</span><br><span class="line">snapshot: <span class="number">5000</span></span><br><span class="line">snapshot_prefix: <span class="string">"examples/mnist/lenet"</span></span><br><span class="line"><span class="comment">//快照。将训练出来的model和solver状态进行保存，snapshot用于设置训练多少次后进行保存，默认为0，不保存。snapshot_prefix设置保存路径。</span></span><br><span class="line"># solver mode: CPU or GPU</span><br><span class="line">solver_mode: GPU  <span class="comment">//设置运行模式。默认为GPU,如果你没有GPU,则需要改成CPU,否则会出错。</span></span><br><span class="line">debug_info: <span class="literal">true</span>  <span class="comment">//调试信息打开，可以看到更多的信息</span></span><br></pre></td></tr></table></figure>
<p>理论大概是这样啦，然后我们尝试用caffe自带的模型训练一下刚才的图片吧。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cp models/bvlc_reference_caffenet/solver.prototxt examples/zclfile/</span><br><span class="line">cp models/bvlc_reference_caffenet/train_val.prototxt examples/zclfile/</span><br><span class="line">vim examples/zclfile/solver.prototxt</span><br><span class="line"><span class="meta">#</span><span class="bash">修改如下：</span></span><br><span class="line">net: "examples/myfile/train_val.prototxt"</span><br><span class="line">test_iter: 2 #总训练集/batch_size</span><br><span class="line">test_interval: 50 #表示训练多少次后进行一次计算准确率</span><br><span class="line">base_lr: 0.001</span><br><span class="line">lr_policy: "step"</span><br><span class="line">gamma: 0.1</span><br><span class="line">stepsize: 100</span><br><span class="line">display: 20 #表示多少次展示一次</span><br><span class="line">max_iter: 500  #表示最多训练多少次</span><br><span class="line">momentum: 0.9</span><br><span class="line">weight_decay: 0.005</span><br><span class="line">solver_mode: GPU</span><br><span class="line">debug_info: true  #调试信息打开，可以看到更多的信息</span><br><span class="line"><span class="meta">#</span><span class="bash">说明：100个测试数据，batch_size为50，因此test_iter设置为2，就能全cover了。在训练过程中，调整学习率，逐步变小。</span></span><br><span class="line">vim examples/zclfile/train_val.prototxt</span><br><span class="line"><span class="meta">#</span><span class="bash">只需要修改两个阶段的data层就可以了，其它可以不用管</span></span><br><span class="line"><span class="meta">#</span><span class="bash">修改两个data layer的mean_file和<span class="built_in">source</span>这两个地方，其它都没有变化</span></span><br></pre></td></tr></table></figure>
<h4 id="5-6-训练和测试"><a href="#5-6-训练和测试" class="headerlink" title="5.6 训练和测试"></a>5.6 训练和测试</h4><p>caffe的运行提供三种接口：c++接口（命令行）、python接口和matlab接口。</p>
<p>我使用的是命令行的方法：</p>
<p><code>./build/tools/caffe train -solver ./examples/zclfile/solver.prototxt</code></p>
<p>生成了：solver_iter_500.caffemodel 和 solver_iter_500.solverstate</p>
<p>上面这个命令是不能生成日志的，若想生成日志：</p>
<p><code>GLOG_logtostderr=0 GLOG_log_dir=&#39;examples/zclfile/&#39; ./build/tools/caffe train -solver ./examples/zclfile/solver.prototxt</code></p>
<p><em>caffemodel</em>文件保存了caffe模型中的参数，<em>solverstate</em>文件保存了训练过程中的一些中间结果</p>
<h4 id="5-7-预测检验"><a href="#5-7-预测检验" class="headerlink" title="5.7 预测检验"></a>5.7 预测检验</h4><p>模型完成训练后，就要对它的训练表现做验证，看看它在其他测试数据集上的正确性。Caffe提供了另外一个功能用于输出测试的结果。以下就是它的脚本：</p>
<p><code>./build/tools/caffe test --model=examples/zclfile/train_val.prototxt --weights=examples/zclfile/solver_iter_500.caffemodel</code></p>
<p>就会生成以下：</p>
<p><img src="/images/image-20190331231311668.png" alt="image-20190331231311668"></p>
<h4 id="5-8-性能测试"><a href="#5-8-性能测试" class="headerlink" title="5.8 性能测试"></a>5.8 性能测试</h4><p><code>./build/tools/caffe time --model=examples/zclfile/train_val.prototxt</code></p>
<p>得到以下结果：</p>
<p><img src="/images/image-20190331231812671.png" alt="image-20190331231812671"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://lelelelelez.github.io/2019/04/02/初识caffe/" data-id="cjv4hpmyr000c7zfygka234g1" class="article-share-link">分享</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习-caffe/">深度学习,caffe</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="/2019/04/12/写在我27岁的这一天/" class="article-nav-link">
        <strong class="article-nav-caption"></strong>
        <div class="article-nav-title">
          
            写在我27岁的这一天
          
        </div>
      </a>
    
    
      <a href="/2019/04/01/MarchSummary/" class="article-nav-link">
        <strong class="article-nav-caption"></strong>
        <div class="article-nav-title">三月小结</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
  <footer class="footer">
  
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2019 记录者</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>Theme  <a href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li>
    </ul>
  </div>
</footer>

</main>
<aside class="sidebar">
  <button class="navbar-toggle"></button>

<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/hexo.svg" alt="记录者"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">主页</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">归档</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">相册</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">关于我</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="Search">
        <i class="fe fe-search"></i>
        搜索
      </a>
    </li>
  </ul>
</nav>

<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
        <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
          <i class="fe fe-feed"></i>
        </a>
      
    </li>
  </ul>
</nav>

<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
</aside>
<script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/lazyload.min.js"></script>
<script src="/js/busuanzi-2.3.pure.min.js"></script>


  <script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/ocean.js"></script>

</body>
</html>